---
layout: post
title:  "Notes on Bandit Algorithm (1): Stochastic Stationary Bandits"
categories: data science
---

Recently, I have been working on a project about evaluation of the _bandit_ algorithms. At Walmart, we uses the _bandit_ model for many interactive systems, for example the campaign page use the bandit algorithm to selects which category of items should be recommended to the customers. It's a powerful tool when your model needs to balance between exploration and exploitation. This model might be considered as intimidating at the first glance as it is deeply theoretically rooted. But the motivations behind it are actually very natural and intuitive. 

Mathematically speaking, the _bandit_ problem is sequential process between the __agent__ and the __environment__. The agent interacts with the enviroment $$n$$ rounds, and at each round the agent needs to take an __action__ $$A_t$$ from the actions space $$\mathcal{A}$$ then it will receive the __reward__ $$R_t \in \mathbb{R} $$. At teach round, the agent is able to make decisions based the observations in the __history__, which is dfined as $$H_{t-1} = ((A_1, R_1), \cdots, (A_{t-1}, R_{t-1}))$$. The agent's decision making is summarized as the __policy__. We denote the policy as $$\pi \in \prod$$. Since realistically, the agent could only see the events happened before, so does his policy. We thus treat the __policy__ as a function that map the $$H_t$$ to $$A_t$$. While the enviroment responds in terms of how the reward is assigned based on the agent's last actions as well as its history. The environment is generally unknown to the agent. It might assume the environment is from some set $$\mathcal{E}$$. In literatures, people usually call $$\mathcal{E}$$ __environment class__. The more we know about the environment, the smaller its space could be. 

Usually, we want the agent to get highest possible cumulative reward in the process. The cumulative reward up to round $$n$$ is defined as $$E[\sum^{n}_{t=1}R_t]$$. The set of all possible policies $$\prod$$ is called __competitor class__. It follows that we can define another way to evaludate the policy, which is the __regret__. Informally, the regret is the expected different between the highest possible rewards and the actual rewards. Suppose we only consider finite number of actions in the set of $$\{1, 2, \cdots, k\}$$. The __stochastic Bernouli bandit__ is an environment with the following conditions. (1) the reward is binary-valued, (2) for each action $$A_t$$,  $$P(R_t=1) = u_a$$, and (3) there are only $$K$$ actions. For this simple case, we might write regret as 
\$$Regret_n = n \max\limits_{a \in \mathcal{A}}u_a - E[\sum^{n}_{t=1}R_t]$$. 
Now the task is how do we learn our policy such that we maximize our expected rewards or minimize the regret. We call it __policy optimization__. 

For those of you who are familiar with reinforcement learning (RL), the terminology used here are quite similar to that of RL. But there is key difference: in _bandit_ problem, __the avaiable actions and the reward for the next round are not affected by previous round__. Beside, the _bandit_ assumes the agent receives rewards at every round of the game, which is not true for some real life examples such as playing the _StarCraft_ game, in which you are not able to see all the actions done by your opponent.

In fact for lots of problems we can even put further constraints to make it easier to solve. one simple but powerful assumtion is that __the reward generated by the environment only depends on the action at the current step__ and it is independent of all previous interactions. We call this type of _bandit_ as __stochastic stationary bandit__, or simply __stochastic bandit__.

A stochastic bandit instance $$\nu=\{P_a: a \in \mathcal{A} \}$$ is a set of reward distributions for each actions. Hence the environment class is represented as 
, \$$ \mathcal{E} = \{ \{P_a: a \in \mathcal{A} \}: P_a \in \mathcal{M}_a, \forall a \in \mathcal{A} \} $$

where the $$ \mathcal{M}_a $$ is the set of bandit instance. The environment class of the above mentioned Bernouli bandit can be written as $$ \{\mathcal{B}(u): u \in [0, 1]^k \}$$. 

So far the definitions presented here are informal, we haven't introduced the underlying probability space of the rewards and actions. There is a specific from of probability space called __canonical bandit model__ which make analysis more conveient. However, the cotent is rather invovled, please refer the Chapter 4.6 of the _bandit book_.

In next post, we will talk about some basic algorithms to solve this type of bandit problem.
